{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dakilaledesma/NCBG/blob/main/train/CNL_Quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RBv2CGN3gJv",
        "outputId": "80ae23d0-4cb2-4f59-946e-08985a8412fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 43.2 ms, sys: 13.7 ms, total: 56.9 ms\n",
            "Wall time: 4.15 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "! unzip -q /content/drive/MyDrive/UNC/NCBG/sept1_classif_training.zip -d /content\n",
        "! cp /content/drive/MyDrive/UNC/NCBG/qry_fsus_trainingdata_ai.xlsx /content/qry_fsus_trainingdata_ai.xlsx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/content/images /content/images\n",
        "! rm -rf /content/content"
      ],
      "metadata": {
        "id": "828eULKmJzax"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"qry_fsus_trainingdata_ai.xlsx\")\n",
        "\n",
        "images = glob(\"images/*.*\")\n",
        "anno_to_fn = {}\n",
        "for im_fn in images:\n",
        "  bn = os.path.basename(im_fn).split(\"__\")[1].replace(\".jpg\", '')\n",
        "  anno_to_fn[f\"{bn}.jpg\"] = im_fn\n",
        "\n",
        "# print(list(anno_to_fn.items())[:5])\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "  anno_bn = row[\"file\"]\n",
        "  try:\n",
        "    im_fn = anno_to_fn[anno_bn]\n",
        "  except KeyError:\n",
        "    pass\n",
        "  im_qual = row[\"imagequality\"]\n",
        "\n",
        "  if not os.path.isdir(f\"qual/{im_qual}\"):\n",
        "    os.makedirs(f\"qual/{im_qual}\")\n",
        "\n",
        "  try:\n",
        "    shutil.move(im_fn, f\"qual/{im_qual}/{idx}_{anno_bn}\")\n",
        "  except FileNotFoundError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "RrYijtrqJnGh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf /content/qual/nan"
      ],
      "metadata": {
        "id": "eWtJ4JI6yE5i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/qual/1.0 /content/qual/0\n",
        "! mv /content/qual/5.0 /content/qual/1\n",
        "! mv /content/qual/10.0 /content/qual/2"
      ],
      "metadata": {
        "id": "Sn-lByx2yM2d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXOHMDgapMby",
        "outputId": "3f529d57-fc59-407e-ab81-88cb1bb01c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-image-models'...\n",
            "remote: Enumerating objects: 11006, done.\u001b[K\n",
            "remote: Counting objects: 100% (533/533), done.\u001b[K\n",
            "remote: Compressing objects: 100% (273/273), done.\u001b[K\n",
            "remote: Total 11006 (delta 333), reused 378 (delta 250), pack-reused 10473\u001b[K\n",
            "Receiving objects: 100% (11006/11006), 20.54 MiB | 22.52 MiB/s, done.\n",
            "Resolving deltas: 100% (8048/8048), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/rwightman/pytorch-image-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "slckThLBilky"
      },
      "outputs": [],
      "source": [
        "import fileinput\n",
        "import sys\n",
        "\n",
        "def replacement(file, previousw, nextw):\n",
        "   for line in fileinput.input(file, inplace=1):\n",
        "       line = line.replace(previousw, nextw)\n",
        "       sys.stdout.write(line)\n",
        "\n",
        "file = \"/content/pytorch-image-models/timm/utils/checkpoint_saver.py\"\n",
        "replacement(file, \"if os.path.exists(last_save_path):\", \"# if os.path.exists(last_save_path):\")\n",
        "replacement(file, \"os.unlink(last_save_path)  # required for Windows support.\", \"# os.unlink(last_save_path)  # required for Windows support.\")\n",
        "replacement(file, \"os.link(last_save_path, save_path)\", \"# os.link(last_save_path, save_path)\")\n",
        "replacement(file, \"os.unlink(best_save_path)\", \"# os.unlink(best_save_path)\")\n",
        "replacement(file, \"os.link(last_save_path, best_save_path)\", \"# os.link(last_save_path, best_save_path)\")\n",
        "replacement(file, \"if os.path.exists(best_save_path):\", \"# if os.path.exists(best_save_path):\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2bjHJwMfqlJ",
        "outputId": "021fab73-fafb-4d20-eb24-1f89981ddee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "\n",
        "print(len(list(glob(\"qual/*\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kIBgHy6pSFD",
        "outputId": "ce756356-6866-46d8-c001-d2ac1ecd5690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  FutureWarning,\n",
            "Training with a single process on 1 GPUs.\n",
            "Loading pretrained weights from url (https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_384.pth)\n",
            "Model convnext_large_384_in22ft1k created, param count:196234947\n",
            "Data processing configuration for current model + dataset:\n",
            "\tinput_size: (3, 600, 600)\n",
            "\tinterpolation: bicubic\n",
            "\tmean: (0.485, 0.456, 0.406)\n",
            "\tstd: (0.229, 0.224, 0.225)\n",
            "\tcrop_pct: 1.0\n",
            "Using native Torch AMP. Training in mixed precision.\n",
            "Scheduled epochs: 80\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Train: 0 [   0/2122 (  0%)]  Loss: 1.132 (1.13)  Time: 8.761s,    0.46/s  (8.761s,    0.46/s)  LR: 1.000e-04  Data: 0.585 (0.585)\n",
            "Train: 0 [  50/2122 (  2%)]  Loss: 0.3409 (0.754)  Time: 0.294s,   13.59/s  (0.452s,    8.85/s)  LR: 1.000e-04  Data: 0.008 (0.017)\n",
            "Train: 0 [ 100/2122 (  5%)]  Loss: 0.4739 (0.773)  Time: 0.288s,   13.87/s  (0.370s,   10.80/s)  LR: 1.000e-04  Data: 0.007 (0.011)\n",
            "Train: 0 [ 150/2122 (  7%)]  Loss: 0.8009 (0.774)  Time: 0.288s,   13.89/s  (0.343s,   11.67/s)  LR: 1.000e-04  Data: 0.007 (0.009)\n",
            "Train: 0 [ 200/2122 (  9%)]  Loss: 0.6879 (0.742)  Time: 0.286s,   14.01/s  (0.329s,   12.17/s)  LR: 1.000e-04  Data: 0.004 (0.008)\n",
            "Train: 0 [ 250/2122 ( 12%)]  Loss: 0.3555 (0.717)  Time: 0.285s,   14.02/s  (0.320s,   12.48/s)  LR: 1.000e-04  Data: 0.004 (0.008)\n",
            "Train: 0 [ 300/2122 ( 14%)]  Loss: 0.3964 (0.710)  Time: 0.285s,   14.01/s  (0.317s,   12.61/s)  LR: 1.000e-04  Data: 0.004 (0.007)\n",
            "Train: 0 [ 350/2122 ( 17%)]  Loss: 0.5978 (0.697)  Time: 0.285s,   14.05/s  (0.313s,   12.78/s)  LR: 1.000e-04  Data: 0.004 (0.007)\n",
            "Train: 0 [ 400/2122 ( 19%)]  Loss: 0.3453 (0.690)  Time: 0.286s,   13.97/s  (0.310s,   12.91/s)  LR: 1.000e-04  Data: 0.004 (0.007)\n",
            "Train: 0 [ 450/2122 ( 21%)]  Loss: 0.4567 (0.681)  Time: 0.285s,   14.03/s  (0.307s,   13.02/s)  LR: 1.000e-04  Data: 0.004 (0.007)\n",
            "Train: 0 [ 500/2122 ( 24%)]  Loss: 0.6472 (0.676)  Time: 0.286s,   13.98/s  (0.305s,   13.10/s)  LR: 1.000e-04  Data: 0.005 (0.007)\n",
            "Train: 0 [ 550/2122 ( 26%)]  Loss: 1.031 (0.667)  Time: 0.286s,   14.00/s  (0.304s,   13.18/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [ 600/2122 ( 28%)]  Loss: 0.6598 (0.663)  Time: 0.286s,   13.98/s  (0.302s,   13.24/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [ 650/2122 ( 31%)]  Loss: 0.8066 (0.670)  Time: 0.285s,   14.02/s  (0.302s,   13.24/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [ 700/2122 ( 33%)]  Loss: 0.7139 (0.667)  Time: 0.287s,   13.92/s  (0.301s,   13.29/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [ 750/2122 ( 35%)]  Loss: 0.5965 (0.665)  Time: 0.286s,   14.01/s  (0.300s,   13.33/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [ 800/2122 ( 38%)]  Loss: 0.8678 (0.665)  Time: 0.286s,   13.96/s  (0.299s,   13.36/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [ 850/2122 ( 40%)]  Loss: 0.4106 (0.662)  Time: 0.291s,   13.76/s  (0.299s,   13.39/s)  LR: 1.000e-04  Data: 0.009 (0.006)\n",
            "Train: 0 [ 900/2122 ( 42%)]  Loss: 1.418 (0.659)  Time: 0.285s,   14.03/s  (0.298s,   13.42/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [ 950/2122 ( 45%)]  Loss: 1.732 (0.661)  Time: 0.318s,   12.58/s  (0.298s,   13.43/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1000/2122 ( 47%)]  Loss: 1.083 (0.660)  Time: 0.286s,   13.97/s  (0.298s,   13.44/s)  LR: 1.000e-04  Data: 0.006 (0.006)\n",
            "Train: 0 [1050/2122 ( 50%)]  Loss: 0.4932 (0.658)  Time: 0.287s,   13.96/s  (0.297s,   13.46/s)  LR: 1.000e-04  Data: 0.006 (0.006)\n",
            "Train: 0 [1100/2122 ( 52%)]  Loss: 0.6759 (0.658)  Time: 0.285s,   14.02/s  (0.297s,   13.48/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1150/2122 ( 54%)]  Loss: 0.7862 (0.658)  Time: 0.286s,   13.97/s  (0.296s,   13.50/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1200/2122 ( 57%)]  Loss: 0.5973 (0.658)  Time: 0.286s,   14.00/s  (0.296s,   13.52/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1250/2122 ( 59%)]  Loss: 0.3625 (0.657)  Time: 0.285s,   14.01/s  (0.296s,   13.53/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [1300/2122 ( 61%)]  Loss: 0.3294 (0.657)  Time: 0.291s,   13.73/s  (0.296s,   13.53/s)  LR: 1.000e-04  Data: 0.008 (0.006)\n",
            "Train: 0 [1350/2122 ( 64%)]  Loss: 0.6999 (0.656)  Time: 0.287s,   13.95/s  (0.295s,   13.54/s)  LR: 1.000e-04  Data: 0.006 (0.006)\n",
            "Train: 0 [1400/2122 ( 66%)]  Loss: 0.8416 (0.655)  Time: 0.286s,   13.99/s  (0.295s,   13.56/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1450/2122 ( 68%)]  Loss: 0.5501 (0.654)  Time: 0.285s,   14.02/s  (0.295s,   13.57/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1500/2122 ( 71%)]  Loss: 0.4314 (0.653)  Time: 0.285s,   14.01/s  (0.295s,   13.58/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1550/2122 ( 73%)]  Loss: 0.9427 (0.653)  Time: 0.289s,   13.85/s  (0.294s,   13.59/s)  LR: 1.000e-04  Data: 0.007 (0.006)\n",
            "Train: 0 [1600/2122 ( 75%)]  Loss: 0.4611 (0.651)  Time: 0.285s,   14.01/s  (0.294s,   13.60/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1650/2122 ( 78%)]  Loss: 0.4708 (0.651)  Time: 0.288s,   13.87/s  (0.294s,   13.59/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1700/2122 ( 80%)]  Loss: 0.3509 (0.651)  Time: 0.288s,   13.90/s  (0.294s,   13.59/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [1750/2122 ( 83%)]  Loss: 1.631 (0.649)  Time: 0.287s,   13.95/s  (0.294s,   13.60/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1800/2122 ( 85%)]  Loss: 0.7434 (0.649)  Time: 0.286s,   14.00/s  (0.294s,   13.61/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [1850/2122 ( 87%)]  Loss: 0.5676 (0.650)  Time: 0.291s,   13.73/s  (0.294s,   13.61/s)  LR: 1.000e-04  Data: 0.009 (0.006)\n",
            "Train: 0 [1900/2122 ( 90%)]  Loss: 0.6690 (0.650)  Time: 0.287s,   13.95/s  (0.294s,   13.62/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [1950/2122 ( 92%)]  Loss: 0.4938 (0.651)  Time: 0.332s,   12.04/s  (0.294s,   13.62/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [2000/2122 ( 94%)]  Loss: 0.6206 (0.652)  Time: 0.286s,   13.97/s  (0.294s,   13.62/s)  LR: 1.000e-04  Data: 0.004 (0.006)\n",
            "Train: 0 [2050/2122 ( 97%)]  Loss: 0.4436 (0.652)  Time: 0.286s,   14.00/s  (0.294s,   13.63/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [2100/2122 ( 99%)]  Loss: 0.8985 (0.652)  Time: 0.288s,   13.91/s  (0.293s,   13.63/s)  LR: 1.000e-04  Data: 0.005 (0.006)\n",
            "Train: 0 [2121/2122 (100%)]  Loss: 1.041 (0.652)  Time: 0.280s,   14.30/s  (0.293s,   13.64/s)  LR: 1.000e-04  Data: 0.000 (0.006)\n",
            "Test: [   0/2121]  Time: 0.543 (0.543)  Loss:  2.4414 (2.4414)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2121]  Time: 0.067 (0.088)  Loss:  0.9575 (2.2945)  Acc@1:  0.0000 ( 1.9608)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 100/2121]  Time: 0.067 (0.083)  Loss:  1.1475 (1.8692)  Acc@1:  0.0000 ( 2.2277)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 150/2121]  Time: 0.074 (0.081)  Loss:  0.8901 (1.6800)  Acc@1: 50.0000 ( 2.8146)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 200/2121]  Time: 0.075 (0.080)  Loss:  1.4082 (1.6103)  Acc@1:  0.0000 ( 2.9851)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 250/2121]  Time: 0.068 (0.079)  Loss:  1.3398 (1.5665)  Acc@1:  0.0000 ( 2.8884)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 300/2121]  Time: 0.069 (0.079)  Loss:  1.2559 (1.5548)  Acc@1: 25.0000 ( 2.6578)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 350/2121]  Time: 0.085 (0.078)  Loss:  1.5527 (1.5505)  Acc@1:  0.0000 ( 2.5641)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 400/2121]  Time: 0.073 (0.078)  Loss:  1.1758 (1.5420)  Acc@1:  0.0000 ( 2.2444)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 450/2121]  Time: 0.072 (0.078)  Loss:  0.1990 (1.5011)  Acc@1: 100.0000 ( 5.8204)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 500/2121]  Time: 0.074 (0.078)  Loss:  0.1127 (1.3697)  Acc@1: 100.0000 (15.1697)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 550/2121]  Time: 0.093 (0.078)  Loss:  0.2876 (1.2579)  Acc@1: 100.0000 (22.8675)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 600/2121]  Time: 0.075 (0.081)  Loss:  0.2339 (1.1669)  Acc@1: 100.0000 (29.2845)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 650/2121]  Time: 0.081 (0.080)  Loss:  0.3518 (1.0947)  Acc@1: 100.0000 (34.7158)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 700/2121]  Time: 0.082 (0.080)  Loss:  0.2363 (1.0330)  Acc@1: 100.0000 (39.3723)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 750/2121]  Time: 0.087 (0.080)  Loss:  0.1755 (0.9822)  Acc@1: 100.0000 (43.4088)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 800/2121]  Time: 0.077 (0.080)  Loss:  0.1238 (0.9311)  Acc@1: 100.0000 (46.9413)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 850/2121]  Time: 0.068 (0.080)  Loss:  0.1281 (0.8853)  Acc@1: 100.0000 (50.0588)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 900/2121]  Time: 0.067 (0.080)  Loss:  0.1912 (0.8447)  Acc@1: 100.0000 (52.8302)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 950/2121]  Time: 0.080 (0.079)  Loss:  0.2639 (0.8087)  Acc@1: 100.0000 (55.3102)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1000/2121]  Time: 0.072 (0.079)  Loss:  0.2174 (0.7771)  Acc@1: 100.0000 (57.5425)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1050/2121]  Time: 0.080 (0.079)  Loss:  0.1863 (0.7498)  Acc@1: 100.0000 (59.5385)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1100/2121]  Time: 0.071 (0.079)  Loss:  0.2573 (0.7234)  Acc@1: 100.0000 (61.3760)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1150/2121]  Time: 0.078 (0.079)  Loss:  0.3389 (0.6995)  Acc@1: 75.0000 (63.0321)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1200/2121]  Time: 0.069 (0.079)  Loss:  0.1146 (0.6785)  Acc@1: 100.0000 (64.5712)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1250/2121]  Time: 0.068 (0.079)  Loss:  0.1318 (0.6577)  Acc@1: 100.0000 (65.9872)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1300/2121]  Time: 0.086 (0.078)  Loss:  0.2314 (0.6385)  Acc@1: 100.0000 (67.2944)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1350/2121]  Time: 0.075 (0.078)  Loss:  0.1304 (0.6209)  Acc@1: 100.0000 (68.5048)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1400/2121]  Time: 0.068 (0.078)  Loss:  0.3462 (0.6045)  Acc@1: 100.0000 (69.6288)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1450/2121]  Time: 0.067 (0.078)  Loss:  0.1476 (0.5897)  Acc@1: 100.0000 (70.6754)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1500/2121]  Time: 0.081 (0.078)  Loss:  0.2705 (0.5758)  Acc@1: 100.0000 (71.6522)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1550/2121]  Time: 0.067 (0.078)  Loss:  0.2383 (0.5634)  Acc@1: 100.0000 (72.5661)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1600/2121]  Time: 0.074 (0.078)  Loss:  0.1064 (0.5517)  Acc@1: 100.0000 (73.4229)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1650/2121]  Time: 0.075 (0.078)  Loss:  0.3884 (0.5406)  Acc@1: 100.0000 (74.2277)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1700/2121]  Time: 0.066 (0.078)  Loss:  0.2181 (0.5309)  Acc@1: 100.0000 (74.9853)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1750/2121]  Time: 0.075 (0.078)  Loss:  0.2876 (0.5232)  Acc@1: 100.0000 (75.6996)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1800/2121]  Time: 0.087 (0.078)  Loss:  0.4895 (0.5158)  Acc@1: 100.0000 (76.3742)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1850/2121]  Time: 0.077 (0.079)  Loss:  0.3384 (0.5079)  Acc@1: 100.0000 (77.0124)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1900/2121]  Time: 0.068 (0.079)  Loss:  0.1523 (0.5012)  Acc@1: 100.0000 (77.6170)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1950/2121]  Time: 0.075 (0.079)  Loss:  0.1316 (0.4939)  Acc@1: 100.0000 (78.1907)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2000/2121]  Time: 0.081 (0.078)  Loss:  0.1302 (0.4852)  Acc@1: 100.0000 (78.7356)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2050/2121]  Time: 0.077 (0.078)  Loss:  0.2360 (0.4770)  Acc@1: 100.0000 (79.2540)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2100/2121]  Time: 0.068 (0.078)  Loss:  0.1565 (0.4694)  Acc@1: 100.0000 (79.7477)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2121/2121]  Time: 0.063 (0.078)  Loss:  0.1115 (0.4660)  Acc@1: 100.0000 (79.9482)  Acc@5: 100.0000 (100.0000)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-0.pth.tar', 79.94816211121584)\n",
            "\n",
            "Train: 1 [   0/2122 (  0%)]  Loss: 1.257 (1.26)  Time: 0.818s,    4.89/s  (0.818s,    4.89/s)  LR: 8.333e-05  Data: 0.388 (0.388)\n",
            "Train: 1 [  50/2122 (  2%)]  Loss: 0.6354 (0.616)  Time: 0.285s,   14.02/s  (0.310s,   12.91/s)  LR: 8.333e-05  Data: 0.004 (0.013)\n",
            "Train: 1 [ 100/2122 (  5%)]  Loss: 0.3566 (0.605)  Time: 0.318s,   12.57/s  (0.305s,   13.10/s)  LR: 8.333e-05  Data: 0.006 (0.010)\n",
            "Train: 1 [ 150/2122 (  7%)]  Loss: 0.5025 (0.625)  Time: 0.299s,   13.36/s  (0.302s,   13.24/s)  LR: 8.333e-05  Data: 0.004 (0.008)\n",
            "Train: 1 [ 200/2122 (  9%)]  Loss: 0.3197 (0.633)  Time: 0.292s,   13.70/s  (0.300s,   13.35/s)  LR: 8.333e-05  Data: 0.007 (0.008)\n",
            "Train: 1 [ 250/2122 ( 12%)]  Loss: 0.7809 (0.634)  Time: 0.285s,   14.03/s  (0.300s,   13.33/s)  LR: 8.333e-05  Data: 0.005 (0.007)\n",
            "Train: 1 [ 300/2122 ( 14%)]  Loss: 0.9963 (0.633)  Time: 0.289s,   13.86/s  (0.298s,   13.42/s)  LR: 8.333e-05  Data: 0.004 (0.007)\n",
            "Train: 1 [ 350/2122 ( 17%)]  Loss: 0.5581 (0.624)  Time: 0.285s,   14.02/s  (0.297s,   13.48/s)  LR: 8.333e-05  Data: 0.004 (0.007)\n",
            "Train: 1 [ 400/2122 ( 19%)]  Loss: 0.4662 (0.619)  Time: 0.287s,   13.92/s  (0.296s,   13.53/s)  LR: 8.333e-05  Data: 0.005 (0.007)\n",
            "Train: 1 [ 450/2122 ( 21%)]  Loss: 0.7935 (0.622)  Time: 0.289s,   13.82/s  (0.295s,   13.57/s)  LR: 8.333e-05  Data: 0.005 (0.007)\n",
            "Train: 1 [ 500/2122 ( 24%)]  Loss: 0.5616 (0.619)  Time: 0.288s,   13.88/s  (0.294s,   13.60/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [ 550/2122 ( 26%)]  Loss: 0.7268 (0.625)  Time: 0.287s,   13.94/s  (0.295s,   13.56/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [ 600/2122 ( 28%)]  Loss: 0.8363 (0.628)  Time: 0.287s,   13.95/s  (0.294s,   13.59/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [ 650/2122 ( 31%)]  Loss: 0.3567 (0.627)  Time: 0.289s,   13.84/s  (0.294s,   13.61/s)  LR: 8.333e-05  Data: 0.007 (0.006)\n",
            "Train: 1 [ 700/2122 ( 33%)]  Loss: 0.3609 (0.626)  Time: 0.285s,   14.04/s  (0.293s,   13.63/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [ 750/2122 ( 35%)]  Loss: 0.3229 (0.623)  Time: 0.286s,   14.01/s  (0.293s,   13.65/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [ 800/2122 ( 38%)]  Loss: 0.7615 (0.624)  Time: 0.288s,   13.89/s  (0.293s,   13.67/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [ 850/2122 ( 40%)]  Loss: 0.3732 (0.623)  Time: 0.286s,   14.01/s  (0.292s,   13.68/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [ 900/2122 ( 42%)]  Loss: 0.3893 (0.623)  Time: 0.286s,   13.96/s  (0.293s,   13.65/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [ 950/2122 ( 45%)]  Loss: 0.6637 (0.623)  Time: 0.290s,   13.78/s  (0.293s,   13.66/s)  LR: 8.333e-05  Data: 0.007 (0.006)\n",
            "Train: 1 [1000/2122 ( 47%)]  Loss: 0.3930 (0.623)  Time: 0.288s,   13.87/s  (0.293s,   13.68/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1050/2122 ( 50%)]  Loss: 0.6011 (0.618)  Time: 0.289s,   13.82/s  (0.292s,   13.68/s)  LR: 8.333e-05  Data: 0.006 (0.006)\n",
            "Train: 1 [1100/2122 ( 52%)]  Loss: 0.3792 (0.617)  Time: 0.289s,   13.84/s  (0.292s,   13.69/s)  LR: 8.333e-05  Data: 0.008 (0.006)\n",
            "Train: 1 [1150/2122 ( 54%)]  Loss: 0.4700 (0.618)  Time: 0.287s,   13.96/s  (0.292s,   13.70/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [1200/2122 ( 57%)]  Loss: 1.257 (0.619)  Time: 0.331s,   12.08/s  (0.292s,   13.69/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [1250/2122 ( 59%)]  Loss: 0.3831 (0.618)  Time: 0.288s,   13.90/s  (0.292s,   13.69/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [1300/2122 ( 61%)]  Loss: 0.5976 (0.615)  Time: 0.286s,   13.98/s  (0.292s,   13.70/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1350/2122 ( 64%)]  Loss: 0.5723 (0.615)  Time: 0.290s,   13.78/s  (0.292s,   13.71/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [1400/2122 ( 66%)]  Loss: 1.037 (0.616)  Time: 0.290s,   13.80/s  (0.292s,   13.71/s)  LR: 8.333e-05  Data: 0.008 (0.006)\n",
            "Train: 1 [1450/2122 ( 68%)]  Loss: 0.6283 (0.615)  Time: 0.285s,   14.01/s  (0.292s,   13.72/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1500/2122 ( 71%)]  Loss: 0.3551 (0.615)  Time: 0.286s,   14.00/s  (0.291s,   13.72/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [1550/2122 ( 73%)]  Loss: 0.3312 (0.614)  Time: 0.286s,   14.00/s  (0.292s,   13.71/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1600/2122 ( 75%)]  Loss: 0.4995 (0.613)  Time: 0.289s,   13.85/s  (0.292s,   13.71/s)  LR: 8.333e-05  Data: 0.007 (0.006)\n",
            "Train: 1 [1650/2122 ( 78%)]  Loss: 0.3488 (0.612)  Time: 0.287s,   13.92/s  (0.292s,   13.72/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1700/2122 ( 80%)]  Loss: 0.4191 (0.613)  Time: 0.288s,   13.87/s  (0.291s,   13.72/s)  LR: 8.333e-05  Data: 0.007 (0.006)\n",
            "Train: 1 [1750/2122 ( 83%)]  Loss: 0.9132 (0.613)  Time: 0.290s,   13.77/s  (0.291s,   13.73/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1800/2122 ( 85%)]  Loss: 0.8848 (0.614)  Time: 0.287s,   13.93/s  (0.291s,   13.73/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [1850/2122 ( 87%)]  Loss: 0.6095 (0.616)  Time: 0.381s,   10.50/s  (0.291s,   13.73/s)  LR: 8.333e-05  Data: 0.013 (0.006)\n",
            "Train: 1 [1900/2122 ( 90%)]  Loss: 0.3588 (0.617)  Time: 0.292s,   13.71/s  (0.292s,   13.72/s)  LR: 8.333e-05  Data: 0.008 (0.006)\n",
            "Train: 1 [1950/2122 ( 92%)]  Loss: 0.3005 (0.616)  Time: 0.288s,   13.90/s  (0.291s,   13.72/s)  LR: 8.333e-05  Data: 0.005 (0.006)\n",
            "Train: 1 [2000/2122 ( 94%)]  Loss: 0.5733 (0.617)  Time: 0.289s,   13.85/s  (0.291s,   13.73/s)  LR: 8.333e-05  Data: 0.007 (0.006)\n",
            "Train: 1 [2050/2122 ( 97%)]  Loss: 0.3647 (0.617)  Time: 0.287s,   13.95/s  (0.291s,   13.73/s)  LR: 8.333e-05  Data: 0.004 (0.006)\n",
            "Train: 1 [2100/2122 ( 99%)]  Loss: 0.4253 (0.617)  Time: 0.291s,   13.76/s  (0.291s,   13.73/s)  LR: 8.333e-05  Data: 0.008 (0.006)\n",
            "Train: 1 [2121/2122 (100%)]  Loss: 0.3933 (0.617)  Time: 0.280s,   14.28/s  (0.291s,   13.74/s)  LR: 8.333e-05  Data: 0.000 (0.006)\n",
            "Test: [   0/2121]  Time: 0.337 (0.337)  Loss:  2.1797 (2.1797)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2121]  Time: 0.089 (0.086)  Loss:  0.4634 (1.9854)  Acc@1: 100.0000 (11.7647)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 100/2121]  Time: 0.072 (0.082)  Loss:  0.6953 (1.5126)  Acc@1: 75.0000 (28.9604)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 150/2121]  Time: 0.078 (0.081)  Loss:  0.4092 (1.3161)  Acc@1: 100.0000 (36.2583)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 200/2121]  Time: 0.118 (0.087)  Loss:  0.8501 (1.2438)  Acc@1: 75.0000 (38.8060)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 250/2121]  Time: 0.076 (0.087)  Loss:  0.9277 (1.2014)  Acc@1: 50.0000 (39.7410)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 300/2121]  Time: 0.077 (0.085)  Loss:  1.1152 (1.2034)  Acc@1: 50.0000 (39.3688)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 350/2121]  Time: 0.077 (0.084)  Loss:  1.3223 (1.2071)  Acc@1: 25.0000 (38.6752)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 400/2121]  Time: 0.083 (0.083)  Loss:  0.7920 (1.2102)  Acc@1: 75.0000 (37.4065)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 450/2121]  Time: 0.072 (0.082)  Loss:  0.2542 (1.1962)  Acc@1: 100.0000 (38.1929)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 500/2121]  Time: 0.073 (0.082)  Loss:  0.0969 (1.1022)  Acc@1: 100.0000 (43.5130)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 550/2121]  Time: 0.067 (0.081)  Loss:  0.3076 (1.0147)  Acc@1: 100.0000 (48.5481)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 600/2121]  Time: 0.083 (0.081)  Loss:  0.3896 (0.9464)  Acc@1: 75.0000 (52.3710)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 650/2121]  Time: 0.077 (0.081)  Loss:  0.4756 (0.8985)  Acc@1: 75.0000 (55.2227)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 700/2121]  Time: 0.079 (0.081)  Loss:  0.2817 (0.8556)  Acc@1: 100.0000 (57.9173)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 750/2121]  Time: 0.087 (0.080)  Loss:  0.1659 (0.8237)  Acc@1: 100.0000 (59.9201)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 800/2121]  Time: 0.079 (0.080)  Loss:  0.1231 (0.7836)  Acc@1: 100.0000 (62.2347)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 850/2121]  Time: 0.087 (0.080)  Loss:  0.0989 (0.7474)  Acc@1: 100.0000 (64.3067)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 900/2121]  Time: 0.068 (0.080)  Loss:  0.2253 (0.7159)  Acc@1: 100.0000 (66.0655)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 950/2121]  Time: 0.073 (0.079)  Loss:  0.4419 (0.6883)  Acc@1: 75.0000 (67.6130)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1000/2121]  Time: 0.087 (0.079)  Loss:  0.3000 (0.6646)  Acc@1: 100.0000 (68.9311)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1050/2121]  Time: 0.085 (0.079)  Loss:  0.2830 (0.6455)  Acc@1: 75.0000 (70.0761)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1100/2121]  Time: 0.076 (0.079)  Loss:  0.3647 (0.6245)  Acc@1: 75.0000 (71.2988)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1150/2121]  Time: 0.068 (0.079)  Loss:  0.5601 (0.6063)  Acc@1: 75.0000 (72.3501)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1200/2121]  Time: 0.079 (0.079)  Loss:  0.1002 (0.5910)  Acc@1: 100.0000 (73.3139)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1250/2121]  Time: 0.076 (0.079)  Loss:  0.1538 (0.5747)  Acc@1: 100.0000 (74.2606)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1300/2121]  Time: 0.072 (0.079)  Loss:  0.3242 (0.5598)  Acc@1: 75.0000 (75.0769)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1350/2121]  Time: 0.075 (0.079)  Loss:  0.1118 (0.5458)  Acc@1: 100.0000 (75.9437)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1400/2121]  Time: 0.080 (0.080)  Loss:  0.4783 (0.5330)  Acc@1: 75.0000 (76.6774)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1450/2121]  Time: 0.067 (0.080)  Loss:  0.1193 (0.5210)  Acc@1: 100.0000 (77.3777)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1500/2121]  Time: 0.088 (0.080)  Loss:  0.2788 (0.5097)  Acc@1: 100.0000 (78.0480)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1550/2121]  Time: 0.082 (0.079)  Loss:  0.3149 (0.5005)  Acc@1: 100.0000 (78.6267)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1600/2121]  Time: 0.069 (0.079)  Loss:  0.0908 (0.4911)  Acc@1: 100.0000 (79.1380)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1650/2121]  Time: 0.073 (0.079)  Loss:  0.6348 (0.4821)  Acc@1: 75.0000 (79.6638)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1700/2121]  Time: 0.068 (0.079)  Loss:  0.2961 (0.4761)  Acc@1: 100.0000 (80.0412)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1750/2121]  Time: 0.068 (0.079)  Loss:  0.3254 (0.4725)  Acc@1: 100.0000 (80.2827)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1800/2121]  Time: 0.078 (0.079)  Loss:  0.9761 (0.4681)  Acc@1: 25.0000 (80.6635)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1850/2121]  Time: 0.094 (0.079)  Loss:  0.3262 (0.4626)  Acc@1: 100.0000 (81.0238)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1900/2121]  Time: 0.078 (0.079)  Loss:  0.1700 (0.4581)  Acc@1: 100.0000 (81.4045)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1950/2121]  Time: 0.076 (0.079)  Loss:  0.1373 (0.4528)  Acc@1: 100.0000 (81.7401)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2000/2121]  Time: 0.074 (0.079)  Loss:  0.1033 (0.4456)  Acc@1: 100.0000 (82.1339)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2050/2121]  Time: 0.075 (0.079)  Loss:  0.3528 (0.4384)  Acc@1: 75.0000 (82.5329)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2100/2121]  Time: 0.076 (0.079)  Loss:  0.1471 (0.4323)  Acc@1: 100.0000 (82.8415)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2121/2121]  Time: 0.063 (0.078)  Loss:  0.1156 (0.4292)  Acc@1: 100.0000 (83.0113)  Acc@5: 100.0000 (100.0000)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-1.pth.tar', 83.01131008482564)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-0.pth.tar', 79.94816211121584)\n",
            "\n",
            "Train: 2 [   0/2122 (  0%)]  Loss: 0.7136 (0.714)  Time: 0.855s,    4.68/s  (0.855s,    4.68/s)  LR: 6.667e-05  Data: 0.442 (0.442)\n",
            "Train: 2 [  50/2122 (  2%)]  Loss: 0.5774 (0.662)  Time: 0.439s,    9.12/s  (0.337s,   11.88/s)  LR: 6.667e-05  Data: 0.019 (0.017)\n",
            "Train: 2 [ 100/2122 (  5%)]  Loss: 0.6392 (0.609)  Time: 0.323s,   12.38/s  (0.385s,   10.40/s)  LR: 6.667e-05  Data: 0.005 (0.014)\n",
            "Train: 2 [ 150/2122 (  7%)]  Loss: 0.6398 (0.620)  Time: 0.321s,   12.45/s  (0.362s,   11.04/s)  LR: 6.667e-05  Data: 0.009 (0.012)\n",
            "Train: 2 [ 200/2122 (  9%)]  Loss: 0.4707 (0.606)  Time: 0.287s,   13.96/s  (0.347s,   11.51/s)  LR: 6.667e-05  Data: 0.004 (0.010)\n",
            "Train: 2 [ 250/2122 ( 12%)]  Loss: 0.4135 (0.610)  Time: 0.286s,   13.99/s  (0.337s,   11.85/s)  LR: 6.667e-05  Data: 0.005 (0.009)\n",
            "Train: 2 [ 300/2122 ( 14%)]  Loss: 0.4159 (0.598)  Time: 0.289s,   13.82/s  (0.329s,   12.15/s)  LR: 6.667e-05  Data: 0.007 (0.009)\n",
            "Train: 2 [ 350/2122 ( 17%)]  Loss: 0.8054 (0.598)  Time: 0.289s,   13.83/s  (0.323s,   12.37/s)  LR: 6.667e-05  Data: 0.008 (0.008)\n",
            "Train: 2 [ 400/2122 ( 19%)]  Loss: 0.5473 (0.594)  Time: 0.287s,   13.91/s  (0.320s,   12.49/s)  LR: 6.667e-05  Data: 0.004 (0.008)\n",
            "Train: 2 [ 450/2122 ( 21%)]  Loss: 0.7314 (0.597)  Time: 0.289s,   13.82/s  (0.317s,   12.63/s)  LR: 6.667e-05  Data: 0.008 (0.008)\n",
            "Train: 2 [ 500/2122 ( 24%)]  Loss: 0.6309 (0.590)  Time: 0.290s,   13.79/s  (0.314s,   12.75/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [ 550/2122 ( 26%)]  Loss: 0.5417 (0.593)  Time: 0.298s,   13.40/s  (0.311s,   12.84/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [ 600/2122 ( 28%)]  Loss: 0.4300 (0.593)  Time: 0.289s,   13.84/s  (0.309s,   12.92/s)  LR: 6.667e-05  Data: 0.008 (0.007)\n",
            "Train: 2 [ 650/2122 ( 31%)]  Loss: 0.2969 (0.590)  Time: 0.286s,   13.99/s  (0.308s,   12.99/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [ 700/2122 ( 33%)]  Loss: 0.5686 (0.590)  Time: 0.285s,   14.02/s  (0.308s,   13.00/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [ 750/2122 ( 35%)]  Loss: 0.3487 (0.589)  Time: 0.285s,   14.05/s  (0.306s,   13.06/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [ 800/2122 ( 38%)]  Loss: 0.5748 (0.590)  Time: 0.288s,   13.88/s  (0.305s,   13.11/s)  LR: 6.667e-05  Data: 0.007 (0.007)\n",
            "Train: 2 [ 850/2122 ( 40%)]  Loss: 0.6445 (0.588)  Time: 0.285s,   14.03/s  (0.304s,   13.15/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [ 900/2122 ( 42%)]  Loss: 0.4003 (0.589)  Time: 0.291s,   13.75/s  (0.303s,   13.19/s)  LR: 6.667e-05  Data: 0.009 (0.007)\n",
            "Train: 2 [ 950/2122 ( 45%)]  Loss: 0.9633 (0.591)  Time: 0.286s,   13.96/s  (0.302s,   13.23/s)  LR: 6.667e-05  Data: 0.004 (0.007)\n",
            "Train: 2 [1000/2122 ( 47%)]  Loss: 0.4911 (0.590)  Time: 0.287s,   13.93/s  (0.302s,   13.23/s)  LR: 6.667e-05  Data: 0.005 (0.007)\n",
            "Train: 2 [1050/2122 ( 50%)]  Loss: 0.4627 (0.593)  Time: 0.286s,   13.97/s  (0.302s,   13.26/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1100/2122 ( 52%)]  Loss: 0.4184 (0.592)  Time: 0.287s,   13.95/s  (0.301s,   13.29/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1150/2122 ( 54%)]  Loss: 0.4784 (0.590)  Time: 0.287s,   13.95/s  (0.300s,   13.32/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1200/2122 ( 57%)]  Loss: 0.3743 (0.592)  Time: 0.290s,   13.81/s  (0.300s,   13.34/s)  LR: 6.667e-05  Data: 0.008 (0.006)\n",
            "Train: 2 [1250/2122 ( 59%)]  Loss: 0.3351 (0.593)  Time: 0.285s,   14.03/s  (0.299s,   13.36/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1300/2122 ( 61%)]  Loss: 0.3037 (0.592)  Time: 0.292s,   13.71/s  (0.299s,   13.38/s)  LR: 6.667e-05  Data: 0.007 (0.006)\n",
            "Train: 2 [1350/2122 ( 64%)]  Loss: 0.5281 (0.591)  Time: 0.286s,   13.97/s  (0.299s,   13.38/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1400/2122 ( 66%)]  Loss: 0.5795 (0.592)  Time: 0.290s,   13.80/s  (0.299s,   13.39/s)  LR: 6.667e-05  Data: 0.008 (0.006)\n",
            "Train: 2 [1450/2122 ( 68%)]  Loss: 1.129 (0.595)  Time: 0.286s,   13.98/s  (0.298s,   13.41/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1500/2122 ( 71%)]  Loss: 0.8043 (0.596)  Time: 0.285s,   14.02/s  (0.298s,   13.43/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1550/2122 ( 73%)]  Loss: 0.4538 (0.595)  Time: 0.286s,   13.98/s  (0.298s,   13.44/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1600/2122 ( 75%)]  Loss: 0.3851 (0.595)  Time: 0.286s,   13.97/s  (0.297s,   13.45/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1650/2122 ( 78%)]  Loss: 0.8928 (0.594)  Time: 0.288s,   13.91/s  (0.298s,   13.45/s)  LR: 6.667e-05  Data: 0.005 (0.006)\n",
            "Train: 2 [1700/2122 ( 80%)]  Loss: 0.4419 (0.595)  Time: 0.286s,   13.98/s  (0.297s,   13.46/s)  LR: 6.667e-05  Data: 0.005 (0.006)\n",
            "Train: 2 [1750/2122 ( 83%)]  Loss: 0.7253 (0.597)  Time: 0.286s,   13.97/s  (0.297s,   13.47/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1800/2122 ( 85%)]  Loss: 0.3057 (0.597)  Time: 0.286s,   13.96/s  (0.297s,   13.48/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [1850/2122 ( 87%)]  Loss: 0.4944 (0.596)  Time: 0.287s,   13.92/s  (0.296s,   13.49/s)  LR: 6.667e-05  Data: 0.005 (0.006)\n",
            "Train: 2 [1900/2122 ( 90%)]  Loss: 1.155 (0.595)  Time: 0.287s,   13.92/s  (0.296s,   13.50/s)  LR: 6.667e-05  Data: 0.005 (0.006)\n",
            "Train: 2 [1950/2122 ( 92%)]  Loss: 0.5763 (0.593)  Time: 0.295s,   13.54/s  (0.296s,   13.49/s)  LR: 6.667e-05  Data: 0.014 (0.006)\n",
            "Train: 2 [2000/2122 ( 94%)]  Loss: 1.815 (0.594)  Time: 0.286s,   13.97/s  (0.296s,   13.50/s)  LR: 6.667e-05  Data: 0.005 (0.006)\n",
            "Train: 2 [2050/2122 ( 97%)]  Loss: 0.6184 (0.595)  Time: 0.287s,   13.95/s  (0.296s,   13.51/s)  LR: 6.667e-05  Data: 0.005 (0.006)\n",
            "Train: 2 [2100/2122 ( 99%)]  Loss: 0.3640 (0.594)  Time: 0.286s,   13.98/s  (0.296s,   13.52/s)  LR: 6.667e-05  Data: 0.004 (0.006)\n",
            "Train: 2 [2121/2122 (100%)]  Loss: 0.3663 (0.594)  Time: 0.282s,   14.18/s  (0.296s,   13.53/s)  LR: 6.667e-05  Data: 0.000 (0.006)\n",
            "Test: [   0/2121]  Time: 0.412 (0.412)  Loss:  1.5684 (1.5684)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2121]  Time: 0.078 (0.087)  Loss:  0.7100 (1.7986)  Acc@1: 75.0000 (18.1373)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 100/2121]  Time: 0.078 (0.082)  Loss:  0.8340 (1.3780)  Acc@1: 50.0000 (31.9307)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 150/2121]  Time: 0.068 (0.081)  Loss:  0.6030 (1.2670)  Acc@1: 75.0000 (32.1192)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 200/2121]  Time: 0.083 (0.080)  Loss:  1.0566 (1.2207)  Acc@1: 25.0000 (31.9652)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 250/2121]  Time: 0.077 (0.080)  Loss:  0.8086 (1.1916)  Acc@1: 25.0000 (32.1713)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 300/2121]  Time: 0.075 (0.079)  Loss:  1.4492 (1.2000)  Acc@1: 25.0000 (30.7309)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 350/2121]  Time: 0.074 (0.079)  Loss:  1.1846 (1.1939)  Acc@1:  0.0000 (30.0570)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 400/2121]  Time: 0.075 (0.078)  Loss:  0.9272 (1.1917)  Acc@1: 50.0000 (27.9925)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 450/2121]  Time: 0.092 (0.078)  Loss:  0.3704 (1.1739)  Acc@1: 100.0000 (29.1574)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 500/2121]  Time: 0.147 (0.082)  Loss:  0.1224 (1.0765)  Acc@1: 100.0000 (36.0279)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 550/2121]  Time: 0.085 (0.082)  Loss:  0.2057 (0.9912)  Acc@1: 100.0000 (41.8330)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 600/2121]  Time: 0.076 (0.082)  Loss:  0.1554 (0.9219)  Acc@1: 100.0000 (46.6722)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 650/2121]  Time: 0.073 (0.081)  Loss:  0.2467 (0.8682)  Acc@1: 100.0000 (50.7296)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 700/2121]  Time: 0.070 (0.081)  Loss:  0.3193 (0.8227)  Acc@1: 100.0000 (54.1726)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 750/2121]  Time: 0.090 (0.081)  Loss:  0.1516 (0.7868)  Acc@1: 100.0000 (56.9907)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 800/2121]  Time: 0.075 (0.080)  Loss:  0.1135 (0.7483)  Acc@1: 100.0000 (59.6754)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 850/2121]  Time: 0.088 (0.080)  Loss:  0.1156 (0.7132)  Acc@1: 100.0000 (62.0447)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 900/2121]  Time: 0.066 (0.080)  Loss:  0.2170 (0.6823)  Acc@1: 100.0000 (64.1232)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 950/2121]  Time: 0.073 (0.080)  Loss:  0.3164 (0.6555)  Acc@1: 75.0000 (65.9569)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1000/2121]  Time: 0.091 (0.080)  Loss:  0.1682 (0.6315)  Acc@1: 100.0000 (67.6074)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1050/2121]  Time: 0.077 (0.080)  Loss:  0.2122 (0.6114)  Acc@1: 100.0000 (69.1484)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1100/2121]  Time: 0.085 (0.079)  Loss:  0.2588 (0.5914)  Acc@1: 100.0000 (70.5495)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1150/2121]  Time: 0.066 (0.079)  Loss:  0.3779 (0.5733)  Acc@1: 75.0000 (71.7420)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1200/2121]  Time: 0.073 (0.079)  Loss:  0.1127 (0.5579)  Acc@1: 100.0000 (72.8560)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1250/2121]  Time: 0.070 (0.079)  Loss:  0.1549 (0.5418)  Acc@1: 100.0000 (73.9408)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1300/2121]  Time: 0.087 (0.079)  Loss:  0.1611 (0.5271)  Acc@1: 100.0000 (74.9424)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1350/2121]  Time: 0.077 (0.079)  Loss:  0.1057 (0.5135)  Acc@1: 100.0000 (75.8697)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1400/2121]  Time: 0.068 (0.079)  Loss:  0.3933 (0.5014)  Acc@1: 100.0000 (76.7309)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1450/2121]  Time: 0.074 (0.079)  Loss:  0.1816 (0.4897)  Acc@1: 100.0000 (77.5155)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1500/2121]  Time: 0.067 (0.079)  Loss:  0.2213 (0.4796)  Acc@1: 100.0000 (78.2645)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1550/2121]  Time: 0.087 (0.079)  Loss:  0.2517 (0.4703)  Acc@1: 100.0000 (78.9491)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1600/2121]  Time: 0.068 (0.078)  Loss:  0.0978 (0.4614)  Acc@1: 100.0000 (79.6065)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1650/2121]  Time: 0.149 (0.079)  Loss:  0.3750 (0.4528)  Acc@1: 100.0000 (80.2241)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1700/2121]  Time: 0.078 (0.079)  Loss:  0.3506 (0.4472)  Acc@1: 75.0000 (80.7025)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1750/2121]  Time: 0.072 (0.079)  Loss:  0.3582 (0.4430)  Acc@1: 100.0000 (81.2107)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1800/2121]  Time: 0.081 (0.079)  Loss:  0.4001 (0.4384)  Acc@1: 100.0000 (81.7324)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1850/2121]  Time: 0.078 (0.079)  Loss:  0.2942 (0.4331)  Acc@1: 100.0000 (82.2258)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1900/2121]  Time: 0.087 (0.079)  Loss:  0.1616 (0.4287)  Acc@1: 100.0000 (82.6933)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1950/2121]  Time: 0.074 (0.079)  Loss:  0.1395 (0.4235)  Acc@1: 100.0000 (83.1240)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2000/2121]  Time: 0.073 (0.079)  Loss:  0.1448 (0.4166)  Acc@1: 100.0000 (83.5457)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2050/2121]  Time: 0.077 (0.079)  Loss:  0.2930 (0.4103)  Acc@1: 75.0000 (83.9347)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2100/2121]  Time: 0.067 (0.079)  Loss:  0.1501 (0.4043)  Acc@1: 100.0000 (84.3170)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2121/2121]  Time: 0.063 (0.079)  Loss:  0.1429 (0.4016)  Acc@1: 100.0000 (84.4722)  Acc@5: 100.0000 (100.0000)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-2.pth.tar', 84.47219604147031)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-1.pth.tar', 83.01131008482564)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-0.pth.tar', 79.94816211121584)\n",
            "\n",
            "Train: 3 [   0/2122 (  0%)]  Loss: 0.7291 (0.729)  Time: 0.911s,    4.39/s  (0.911s,    4.39/s)  LR: 4.983e-05  Data: 0.457 (0.457)\n",
            "Train: 3 [  50/2122 (  2%)]  Loss: 0.6887 (0.524)  Time: 0.417s,    9.60/s  (0.362s,   11.05/s)  LR: 4.983e-05  Data: 0.006 (0.017)\n",
            "Train: 3 [ 100/2122 (  5%)]  Loss: 0.6032 (0.572)  Time: 0.288s,   13.91/s  (0.359s,   11.15/s)  LR: 4.983e-05  Data: 0.005 (0.013)\n",
            "Train: 3 [ 150/2122 (  7%)]  Loss: 0.3100 (0.570)  Time: 0.287s,   13.95/s  (0.346s,   11.57/s)  LR: 4.983e-05  Data: 0.006 (0.011)\n",
            "Train: 3 [ 200/2122 (  9%)]  Loss: 0.4768 (0.582)  Time: 0.309s,   12.95/s  (0.334s,   11.97/s)  LR: 4.983e-05  Data: 0.007 (0.010)\n",
            "Train: 3 [ 250/2122 ( 12%)]  Loss: 0.5682 (0.567)  Time: 0.310s,   12.89/s  (0.328s,   12.21/s)  LR: 4.983e-05  Data: 0.004 (0.009)\n",
            "Train: 3 [ 300/2122 ( 14%)]  Loss: 0.3047 (0.560)  Time: 0.300s,   13.31/s  (0.323s,   12.37/s)  LR: 4.983e-05  Data: 0.006 (0.008)\n",
            "Train: 3 [ 350/2122 ( 17%)]  Loss: 0.4161 (0.564)  Time: 0.288s,   13.89/s  (0.319s,   12.55/s)  LR: 4.983e-05  Data: 0.005 (0.008)\n",
            "Train: 3 [ 400/2122 ( 19%)]  Loss: 0.4263 (0.563)  Time: 0.286s,   13.98/s  (0.315s,   12.70/s)  LR: 4.983e-05  Data: 0.004 (0.008)\n",
            "Train: 3 [ 450/2122 ( 21%)]  Loss: 0.5266 (0.560)  Time: 0.289s,   13.85/s  (0.314s,   12.75/s)  LR: 4.983e-05  Data: 0.004 (0.008)\n",
            "Train: 3 [ 500/2122 ( 24%)]  Loss: 0.3840 (0.557)  Time: 0.286s,   14.00/s  (0.311s,   12.86/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 550/2122 ( 26%)]  Loss: 0.3954 (0.555)  Time: 0.288s,   13.89/s  (0.309s,   12.95/s)  LR: 4.983e-05  Data: 0.006 (0.007)\n",
            "Train: 3 [ 600/2122 ( 28%)]  Loss: 0.8742 (0.558)  Time: 0.288s,   13.89/s  (0.307s,   13.02/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 650/2122 ( 31%)]  Loss: 0.3443 (0.559)  Time: 0.288s,   13.90/s  (0.306s,   13.09/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 700/2122 ( 33%)]  Loss: 0.8147 (0.560)  Time: 0.287s,   13.96/s  (0.304s,   13.15/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 750/2122 ( 35%)]  Loss: 0.5683 (0.561)  Time: 0.338s,   11.83/s  (0.304s,   13.16/s)  LR: 4.983e-05  Data: 0.020 (0.007)\n",
            "Train: 3 [ 800/2122 ( 38%)]  Loss: 0.3240 (0.559)  Time: 0.289s,   13.86/s  (0.303s,   13.19/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 850/2122 ( 40%)]  Loss: 0.9522 (0.559)  Time: 0.286s,   13.98/s  (0.302s,   13.23/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 900/2122 ( 42%)]  Loss: 0.2943 (0.560)  Time: 0.286s,   14.00/s  (0.302s,   13.26/s)  LR: 4.983e-05  Data: 0.005 (0.007)\n",
            "Train: 3 [ 950/2122 ( 45%)]  Loss: 0.5219 (0.559)  Time: 0.286s,   13.99/s  (0.301s,   13.29/s)  LR: 4.983e-05  Data: 0.004 (0.007)\n",
            "Train: 3 [1000/2122 ( 47%)]  Loss: 0.3011 (0.558)  Time: 0.286s,   14.00/s  (0.300s,   13.32/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1050/2122 ( 50%)]  Loss: 0.8489 (0.558)  Time: 0.291s,   13.77/s  (0.300s,   13.35/s)  LR: 4.983e-05  Data: 0.004 (0.006)\n",
            "Train: 3 [1100/2122 ( 52%)]  Loss: 0.9499 (0.556)  Time: 0.288s,   13.90/s  (0.300s,   13.34/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1150/2122 ( 54%)]  Loss: 0.3666 (0.554)  Time: 0.297s,   13.46/s  (0.299s,   13.36/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1200/2122 ( 57%)]  Loss: 0.6132 (0.556)  Time: 0.290s,   13.82/s  (0.299s,   13.38/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1250/2122 ( 59%)]  Loss: 0.4896 (0.556)  Time: 0.286s,   13.98/s  (0.298s,   13.40/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1300/2122 ( 61%)]  Loss: 0.3949 (0.553)  Time: 0.290s,   13.82/s  (0.298s,   13.42/s)  LR: 4.983e-05  Data: 0.008 (0.006)\n",
            "Train: 3 [1350/2122 ( 64%)]  Loss: 0.5275 (0.553)  Time: 0.286s,   13.97/s  (0.298s,   13.43/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1400/2122 ( 66%)]  Loss: 0.7289 (0.553)  Time: 0.288s,   13.89/s  (0.298s,   13.42/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1450/2122 ( 68%)]  Loss: 0.3259 (0.553)  Time: 0.298s,   13.42/s  (0.298s,   13.44/s)  LR: 4.983e-05  Data: 0.008 (0.006)\n",
            "Train: 3 [1500/2122 ( 71%)]  Loss: 0.6156 (0.555)  Time: 0.287s,   13.94/s  (0.297s,   13.45/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1550/2122 ( 73%)]  Loss: 0.4321 (0.555)  Time: 0.286s,   13.97/s  (0.297s,   13.47/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1600/2122 ( 75%)]  Loss: 0.4984 (0.555)  Time: 0.287s,   13.93/s  (0.297s,   13.48/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1650/2122 ( 78%)]  Loss: 0.3967 (0.552)  Time: 0.287s,   13.93/s  (0.296s,   13.49/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1700/2122 ( 80%)]  Loss: 0.7666 (0.553)  Time: 0.288s,   13.90/s  (0.297s,   13.48/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1750/2122 ( 83%)]  Loss: 0.5892 (0.551)  Time: 0.286s,   13.98/s  (0.296s,   13.49/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1800/2122 ( 85%)]  Loss: 0.7393 (0.552)  Time: 0.289s,   13.86/s  (0.296s,   13.50/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1850/2122 ( 87%)]  Loss: 1.016 (0.552)  Time: 0.291s,   13.76/s  (0.296s,   13.51/s)  LR: 4.983e-05  Data: 0.009 (0.006)\n",
            "Train: 3 [1900/2122 ( 90%)]  Loss: 0.6840 (0.553)  Time: 0.287s,   13.94/s  (0.296s,   13.52/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [1950/2122 ( 92%)]  Loss: 0.7257 (0.553)  Time: 0.286s,   13.98/s  (0.296s,   13.53/s)  LR: 4.983e-05  Data: 0.004 (0.006)\n",
            "Train: 3 [2000/2122 ( 94%)]  Loss: 0.4404 (0.553)  Time: 0.368s,   10.87/s  (0.296s,   13.53/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [2050/2122 ( 97%)]  Loss: 0.5078 (0.552)  Time: 0.291s,   13.74/s  (0.296s,   13.53/s)  LR: 4.983e-05  Data: 0.008 (0.006)\n",
            "Train: 3 [2100/2122 ( 99%)]  Loss: 0.4017 (0.552)  Time: 0.289s,   13.84/s  (0.295s,   13.54/s)  LR: 4.983e-05  Data: 0.005 (0.006)\n",
            "Train: 3 [2121/2122 (100%)]  Loss: 0.4836 (0.552)  Time: 0.281s,   14.22/s  (0.295s,   13.54/s)  LR: 4.983e-05  Data: 0.000 (0.006)\n",
            "Test: [   0/2121]  Time: 0.387 (0.387)  Loss:  0.5977 (0.5977)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2121]  Time: 0.075 (0.087)  Loss:  0.4722 (1.2918)  Acc@1: 100.0000 (35.7843)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 100/2121]  Time: 0.069 (0.083)  Loss:  0.3416 (0.9988)  Acc@1: 100.0000 (53.4653)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 150/2121]  Time: 0.069 (0.082)  Loss:  0.4573 (0.8990)  Acc@1: 100.0000 (59.4371)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 200/2121]  Time: 0.076 (0.081)  Loss:  0.7188 (0.8646)  Acc@1: 50.0000 (60.6965)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 250/2121]  Time: 0.074 (0.081)  Loss:  0.3452 (0.8403)  Acc@1: 100.0000 (62.8486)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 300/2121]  Time: 0.068 (0.080)  Loss:  0.9243 (0.8504)  Acc@1: 75.0000 (61.5449)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 350/2121]  Time: 0.067 (0.079)  Loss:  1.0312 (0.8740)  Acc@1: 50.0000 (59.9003)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 400/2121]  Time: 0.068 (0.079)  Loss:  0.5908 (0.8807)  Acc@1: 100.0000 (58.8529)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 450/2121]  Time: 0.073 (0.079)  Loss:  0.1077 (0.8876)  Acc@1: 100.0000 (58.3703)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 500/2121]  Time: 0.076 (0.079)  Loss:  0.0651 (0.8131)  Acc@1: 100.0000 (62.1756)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 550/2121]  Time: 0.072 (0.079)  Loss:  0.1492 (0.7467)  Acc@1: 100.0000 (65.6080)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 600/2121]  Time: 0.077 (0.079)  Loss:  0.1450 (0.6935)  Acc@1: 100.0000 (68.3444)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 650/2121]  Time: 0.068 (0.079)  Loss:  0.1858 (0.6544)  Acc@1: 100.0000 (70.5069)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 700/2121]  Time: 0.077 (0.082)  Loss:  0.3047 (0.6217)  Acc@1: 100.0000 (72.3609)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 750/2121]  Time: 0.066 (0.082)  Loss:  0.0767 (0.5970)  Acc@1: 100.0000 (73.6352)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 800/2121]  Time: 0.074 (0.081)  Loss:  0.0620 (0.5667)  Acc@1: 100.0000 (75.1873)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 850/2121]  Time: 0.083 (0.081)  Loss:  0.0603 (0.5389)  Acc@1: 100.0000 (76.6157)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 900/2121]  Time: 0.081 (0.081)  Loss:  0.1015 (0.5154)  Acc@1: 100.0000 (77.7469)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 950/2121]  Time: 0.079 (0.081)  Loss:  0.5195 (0.4959)  Acc@1: 75.0000 (78.7329)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1000/2121]  Time: 0.075 (0.080)  Loss:  0.1241 (0.4781)  Acc@1: 100.0000 (79.6953)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1050/2121]  Time: 0.082 (0.080)  Loss:  0.1565 (0.4626)  Acc@1: 100.0000 (80.4710)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1100/2121]  Time: 0.069 (0.080)  Loss:  0.3008 (0.4472)  Acc@1: 75.0000 (81.2897)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1150/2121]  Time: 0.068 (0.080)  Loss:  0.3909 (0.4332)  Acc@1: 75.0000 (82.0156)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1200/2121]  Time: 0.089 (0.080)  Loss:  0.0625 (0.4217)  Acc@1: 100.0000 (82.5770)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1250/2121]  Time: 0.067 (0.080)  Loss:  0.0756 (0.4089)  Acc@1: 100.0000 (83.2734)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1300/2121]  Time: 0.073 (0.080)  Loss:  0.1133 (0.3976)  Acc@1: 100.0000 (83.8586)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1350/2121]  Time: 0.076 (0.080)  Loss:  0.0806 (0.3873)  Acc@1: 100.0000 (84.3819)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1400/2121]  Time: 0.067 (0.080)  Loss:  0.4131 (0.3782)  Acc@1: 100.0000 (84.8680)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1450/2121]  Time: 0.072 (0.079)  Loss:  0.0677 (0.3692)  Acc@1: 100.0000 (85.3205)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1500/2121]  Time: 0.078 (0.079)  Loss:  0.2108 (0.3609)  Acc@1: 100.0000 (85.7595)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1550/2121]  Time: 0.097 (0.079)  Loss:  0.1910 (0.3535)  Acc@1: 100.0000 (86.1541)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1600/2121]  Time: 0.077 (0.079)  Loss:  0.0524 (0.3465)  Acc@1: 100.0000 (86.5240)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1650/2121]  Time: 0.069 (0.079)  Loss:  0.3809 (0.3398)  Acc@1: 100.0000 (86.9019)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1700/2121]  Time: 0.072 (0.079)  Loss:  0.3577 (0.3358)  Acc@1: 75.0000 (87.1693)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1750/2121]  Time: 0.075 (0.079)  Loss:  0.4304 (0.3332)  Acc@1: 100.0000 (87.3215)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1800/2121]  Time: 0.094 (0.079)  Loss:  0.2949 (0.3298)  Acc@1: 100.0000 (87.6041)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1850/2121]  Time: 0.069 (0.080)  Loss:  0.1694 (0.3256)  Acc@1: 100.0000 (87.8849)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1900/2121]  Time: 0.068 (0.080)  Loss:  0.0967 (0.3222)  Acc@1: 100.0000 (88.1378)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1950/2121]  Time: 0.085 (0.080)  Loss:  0.0624 (0.3186)  Acc@1: 100.0000 (88.3393)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2000/2121]  Time: 0.081 (0.080)  Loss:  0.0813 (0.3128)  Acc@1: 100.0000 (88.6182)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2050/2121]  Time: 0.068 (0.079)  Loss:  0.4097 (0.3075)  Acc@1: 75.0000 (88.8835)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2100/2121]  Time: 0.067 (0.079)  Loss:  0.0714 (0.3026)  Acc@1: 100.0000 (89.1480)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2121/2121]  Time: 0.063 (0.079)  Loss:  0.0693 (0.3003)  Acc@1: 100.0000 (89.2436)  Acc@5: 100.0000 (100.0000)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-3.pth.tar', 89.24363807728558)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-2.pth.tar', 84.47219604147031)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-1.pth.tar', 83.01131008482564)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-0.pth.tar', 79.94816211121584)\n",
            "\n",
            "Train: 4 [   0/2122 (  0%)]  Loss: 0.3906 (0.391)  Time: 0.896s,    4.46/s  (0.896s,    4.46/s)  LR: 4.970e-05  Data: 0.481 (0.481)\n",
            "Train: 4 [  50/2122 (  2%)]  Loss: 0.3185 (0.525)  Time: 0.379s,   10.56/s  (0.347s,   11.52/s)  LR: 4.970e-05  Data: 0.021 (0.017)\n",
            "Train: 4 [ 100/2122 (  5%)]  Loss: 0.5243 (0.526)  Time: 0.294s,   13.61/s  (0.357s,   11.19/s)  LR: 4.970e-05  Data: 0.007 (0.013)\n",
            "Train: 4 [ 150/2122 (  7%)]  Loss: 0.7486 (0.507)  Time: 0.303s,   13.21/s  (0.338s,   11.82/s)  LR: 4.970e-05  Data: 0.005 (0.010)\n",
            "Train: 4 [ 200/2122 (  9%)]  Loss: 0.7413 (0.502)  Time: 0.289s,   13.84/s  (0.335s,   11.93/s)  LR: 4.970e-05  Data: 0.008 (0.010)\n",
            "Train: 4 [ 250/2122 ( 12%)]  Loss: 0.5436 (0.495)  Time: 0.310s,   12.92/s  (0.329s,   12.17/s)  LR: 4.970e-05  Data: 0.006 (0.009)\n",
            "Train: 4 [ 300/2122 ( 14%)]  Loss: 0.3547 (0.493)  Time: 0.288s,   13.88/s  (0.323s,   12.38/s)  LR: 4.970e-05  Data: 0.006 (0.008)\n",
            "Train: 4 [ 350/2122 ( 17%)]  Loss: 0.4002 (0.495)  Time: 0.286s,   14.00/s  (0.319s,   12.55/s)  LR: 4.970e-05  Data: 0.005 (0.008)\n",
            "Train: 4 [ 400/2122 ( 19%)]  Loss: 0.3798 (0.493)  Time: 0.293s,   13.67/s  (0.315s,   12.71/s)  LR: 4.970e-05  Data: 0.005 (0.008)\n",
            "Train: 4 [ 450/2122 ( 21%)]  Loss: 0.3393 (0.494)  Time: 0.290s,   13.80/s  (0.312s,   12.83/s)  LR: 4.970e-05  Data: 0.008 (0.008)\n",
            "Train: 4 [ 500/2122 ( 24%)]  Loss: 1.076 (0.496)  Time: 0.286s,   14.00/s  (0.311s,   12.86/s)  LR: 4.970e-05  Data: 0.004 (0.007)\n",
            "Train: 4 [ 550/2122 ( 26%)]  Loss: 0.7385 (0.499)  Time: 0.291s,   13.75/s  (0.309s,   12.95/s)  LR: 4.970e-05  Data: 0.005 (0.007)\n",
            "Train: 4 [ 600/2122 ( 28%)]  Loss: 0.6450 (0.499)  Time: 0.288s,   13.90/s  (0.307s,   13.02/s)  LR: 4.970e-05  Data: 0.005 (0.007)\n",
            "Train: 4 [ 650/2122 ( 31%)]  Loss: 0.5029 (0.502)  Time: 0.286s,   14.00/s  (0.306s,   13.09/s)  LR: 4.970e-05  Data: 0.005 (0.007)\n",
            "Train: 4 [ 700/2122 ( 33%)]  Loss: 0.3675 (0.501)  Time: 0.285s,   14.01/s  (0.304s,   13.14/s)  LR: 4.970e-05  Data: 0.004 (0.007)\n",
            "Train: 4 [ 750/2122 ( 35%)]  Loss: 0.3969 (0.501)  Time: 0.289s,   13.83/s  (0.303s,   13.19/s)  LR: 4.970e-05  Data: 0.008 (0.007)\n",
            "Train: 4 [ 800/2122 ( 38%)]  Loss: 0.8320 (0.500)  Time: 0.287s,   13.93/s  (0.303s,   13.19/s)  LR: 4.970e-05  Data: 0.005 (0.007)\n",
            "Train: 4 [ 850/2122 ( 40%)]  Loss: 0.3932 (0.502)  Time: 0.287s,   13.92/s  (0.302s,   13.23/s)  LR: 4.970e-05  Data: 0.005 (0.007)\n",
            "Train: 4 [ 900/2122 ( 42%)]  Loss: 0.3255 (0.502)  Time: 0.289s,   13.82/s  (0.301s,   13.27/s)  LR: 4.970e-05  Data: 0.008 (0.006)\n",
            "Train: 4 [ 950/2122 ( 45%)]  Loss: 0.7308 (0.504)  Time: 0.291s,   13.74/s  (0.301s,   13.30/s)  LR: 4.970e-05  Data: 0.009 (0.006)\n",
            "Train: 4 [1000/2122 ( 47%)]  Loss: 0.4079 (0.502)  Time: 0.292s,   13.68/s  (0.300s,   13.33/s)  LR: 4.970e-05  Data: 0.008 (0.006)\n",
            "Train: 4 [1050/2122 ( 50%)]  Loss: 0.4138 (0.503)  Time: 0.286s,   14.00/s  (0.300s,   13.35/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1100/2122 ( 52%)]  Loss: 0.4633 (0.505)  Time: 0.292s,   13.68/s  (0.300s,   13.35/s)  LR: 4.970e-05  Data: 0.012 (0.006)\n",
            "Train: 4 [1150/2122 ( 54%)]  Loss: 0.7886 (0.506)  Time: 0.289s,   13.85/s  (0.299s,   13.37/s)  LR: 4.970e-05  Data: 0.008 (0.006)\n",
            "Train: 4 [1200/2122 ( 57%)]  Loss: 0.4944 (0.505)  Time: 0.286s,   13.99/s  (0.299s,   13.40/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1250/2122 ( 59%)]  Loss: 0.3003 (0.505)  Time: 0.286s,   14.00/s  (0.298s,   13.42/s)  LR: 4.970e-05  Data: 0.004 (0.006)\n",
            "Train: 4 [1300/2122 ( 61%)]  Loss: 0.5133 (0.506)  Time: 0.286s,   13.97/s  (0.298s,   13.43/s)  LR: 4.970e-05  Data: 0.004 (0.006)\n",
            "Train: 4 [1350/2122 ( 64%)]  Loss: 0.4705 (0.508)  Time: 0.286s,   13.98/s  (0.297s,   13.45/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1400/2122 ( 66%)]  Loss: 1.304 (0.508)  Time: 0.340s,   11.76/s  (0.298s,   13.44/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1450/2122 ( 68%)]  Loss: 0.7142 (0.509)  Time: 0.286s,   13.97/s  (0.297s,   13.46/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1500/2122 ( 71%)]  Loss: 0.3384 (0.508)  Time: 0.286s,   14.01/s  (0.297s,   13.47/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1550/2122 ( 73%)]  Loss: 0.5333 (0.507)  Time: 0.287s,   13.92/s  (0.297s,   13.49/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1600/2122 ( 75%)]  Loss: 0.5744 (0.509)  Time: 0.290s,   13.81/s  (0.296s,   13.50/s)  LR: 4.970e-05  Data: 0.008 (0.006)\n",
            "Train: 4 [1650/2122 ( 78%)]  Loss: 0.3553 (0.509)  Time: 0.286s,   13.98/s  (0.296s,   13.51/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1700/2122 ( 80%)]  Loss: 0.4691 (0.510)  Time: 0.321s,   12.46/s  (0.296s,   13.52/s)  LR: 4.970e-05  Data: 0.017 (0.006)\n",
            "Train: 4 [1750/2122 ( 83%)]  Loss: 0.7589 (0.510)  Time: 0.295s,   13.55/s  (0.296s,   13.51/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1800/2122 ( 85%)]  Loss: 0.7530 (0.510)  Time: 0.288s,   13.89/s  (0.296s,   13.52/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1850/2122 ( 87%)]  Loss: 0.3508 (0.510)  Time: 0.288s,   13.89/s  (0.296s,   13.53/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [1900/2122 ( 90%)]  Loss: 0.4910 (0.510)  Time: 0.287s,   13.94/s  (0.295s,   13.54/s)  LR: 4.970e-05  Data: 0.006 (0.006)\n",
            "Train: 4 [1950/2122 ( 92%)]  Loss: 0.7431 (0.511)  Time: 0.287s,   13.94/s  (0.295s,   13.55/s)  LR: 4.970e-05  Data: 0.006 (0.006)\n",
            "Train: 4 [2000/2122 ( 94%)]  Loss: 0.3909 (0.511)  Time: 0.288s,   13.87/s  (0.295s,   13.56/s)  LR: 4.970e-05  Data: 0.005 (0.006)\n",
            "Train: 4 [2050/2122 ( 97%)]  Loss: 0.3547 (0.512)  Time: 0.291s,   13.75/s  (0.295s,   13.57/s)  LR: 4.970e-05  Data: 0.007 (0.006)\n",
            "Train: 4 [2100/2122 ( 99%)]  Loss: 0.3492 (0.512)  Time: 0.287s,   13.93/s  (0.295s,   13.57/s)  LR: 4.970e-05  Data: 0.006 (0.006)\n",
            "Train: 4 [2121/2122 (100%)]  Loss: 0.4572 (0.513)  Time: 0.281s,   14.23/s  (0.295s,   13.58/s)  LR: 4.970e-05  Data: 0.000 (0.006)\n",
            "Test: [   0/2121]  Time: 0.400 (0.400)  Loss:  0.8672 (0.8672)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [  50/2121]  Time: 0.084 (0.087)  Loss:  0.4355 (0.6928)  Acc@1: 100.0000 (71.0784)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 100/2121]  Time: 0.096 (0.083)  Loss:  0.3110 (0.5637)  Acc@1: 100.0000 (78.9604)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 150/2121]  Time: 0.068 (0.082)  Loss:  0.5645 (0.5491)  Acc@1: 75.0000 (80.1325)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 200/2121]  Time: 0.080 (0.082)  Loss:  0.3079 (0.5372)  Acc@1: 100.0000 (80.2239)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 250/2121]  Time: 0.067 (0.082)  Loss:  0.1576 (0.5228)  Acc@1: 100.0000 (81.3745)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 300/2121]  Time: 0.082 (0.081)  Loss:  0.5962 (0.5378)  Acc@1: 75.0000 (80.0664)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 350/2121]  Time: 0.076 (0.080)  Loss:  0.9399 (0.5573)  Acc@1: 75.0000 (78.4900)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 400/2121]  Time: 0.073 (0.080)  Loss:  0.3594 (0.5563)  Acc@1: 100.0000 (78.2419)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 450/2121]  Time: 0.076 (0.080)  Loss:  0.1769 (0.5603)  Acc@1: 100.0000 (77.7716)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 500/2121]  Time: 0.093 (0.080)  Loss:  0.0898 (0.5205)  Acc@1: 100.0000 (79.7405)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 550/2121]  Time: 0.084 (0.080)  Loss:  0.3267 (0.4838)  Acc@1: 75.0000 (81.4882)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 600/2121]  Time: 0.082 (0.080)  Loss:  0.1187 (0.4538)  Acc@1: 100.0000 (82.9451)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 650/2121]  Time: 0.067 (0.080)  Loss:  0.1659 (0.4333)  Acc@1: 100.0000 (84.0246)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 700/2121]  Time: 0.075 (0.080)  Loss:  0.5781 (0.4179)  Acc@1: 50.0000 (84.9144)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 750/2121]  Time: 0.075 (0.080)  Loss:  0.1296 (0.4083)  Acc@1: 100.0000 (85.2863)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 800/2121]  Time: 0.072 (0.079)  Loss:  0.0952 (0.3905)  Acc@1: 100.0000 (86.1735)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 850/2121]  Time: 0.073 (0.079)  Loss:  0.0884 (0.3742)  Acc@1: 100.0000 (86.9859)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 900/2121]  Time: 0.071 (0.079)  Loss:  0.0935 (0.3607)  Acc@1: 100.0000 (87.5694)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [ 950/2121]  Time: 0.068 (0.079)  Loss:  0.6060 (0.3498)  Acc@1: 75.0000 (88.0915)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1000/2121]  Time: 0.074 (0.079)  Loss:  0.1606 (0.3407)  Acc@1: 100.0000 (88.5614)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1050/2121]  Time: 0.084 (0.079)  Loss:  0.1648 (0.3336)  Acc@1: 100.0000 (88.9153)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1100/2121]  Time: 0.082 (0.079)  Loss:  0.3259 (0.3254)  Acc@1: 75.0000 (89.3052)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1150/2121]  Time: 0.068 (0.079)  Loss:  0.3318 (0.3175)  Acc@1: 75.0000 (89.6829)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1200/2121]  Time: 0.103 (0.079)  Loss:  0.0926 (0.3112)  Acc@1: 100.0000 (89.9875)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1250/2121]  Time: 0.080 (0.079)  Loss:  0.1287 (0.3043)  Acc@1: 100.0000 (90.3477)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1300/2121]  Time: 0.074 (0.079)  Loss:  0.0760 (0.2978)  Acc@1: 100.0000 (90.6610)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1350/2121]  Time: 0.071 (0.079)  Loss:  0.1587 (0.2916)  Acc@1: 100.0000 (90.9511)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1400/2121]  Time: 0.068 (0.079)  Loss:  0.2952 (0.2873)  Acc@1: 100.0000 (91.1670)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1450/2121]  Time: 0.071 (0.078)  Loss:  0.1490 (0.2823)  Acc@1: 100.0000 (91.4197)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1500/2121]  Time: 0.078 (0.078)  Loss:  0.1450 (0.2779)  Acc@1: 100.0000 (91.6722)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1550/2121]  Time: 0.088 (0.078)  Loss:  0.2361 (0.2737)  Acc@1: 100.0000 (91.8923)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1600/2121]  Time: 0.085 (0.078)  Loss:  0.0653 (0.2704)  Acc@1: 100.0000 (92.0987)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1650/2121]  Time: 0.069 (0.078)  Loss:  0.5669 (0.2673)  Acc@1: 75.0000 (92.2925)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1700/2121]  Time: 0.083 (0.078)  Loss:  0.2234 (0.2655)  Acc@1: 100.0000 (92.4015)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1750/2121]  Time: 0.075 (0.078)  Loss:  0.4607 (0.2647)  Acc@1: 75.0000 (92.4615)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1800/2121]  Time: 0.068 (0.078)  Loss:  0.4102 (0.2638)  Acc@1: 100.0000 (92.5736)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1850/2121]  Time: 0.074 (0.078)  Loss:  0.2275 (0.2624)  Acc@1: 100.0000 (92.7202)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1900/2121]  Time: 0.080 (0.078)  Loss:  0.1595 (0.2620)  Acc@1: 100.0000 (92.8327)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [1950/2121]  Time: 0.085 (0.078)  Loss:  0.1024 (0.2606)  Acc@1: 100.0000 (92.9523)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2000/2121]  Time: 0.079 (0.078)  Loss:  0.2207 (0.2575)  Acc@1: 100.0000 (93.1159)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2050/2121]  Time: 0.080 (0.078)  Loss:  0.4363 (0.2548)  Acc@1: 75.0000 (93.2716)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2100/2121]  Time: 0.075 (0.078)  Loss:  0.1077 (0.2521)  Acc@1: 100.0000 (93.4198)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [2121/2121]  Time: 0.063 (0.078)  Loss:  0.1057 (0.2507)  Acc@1: 100.0000 (93.4849)  Acc@5: 100.0000 (100.0000)\n",
            "Current checkpoints:\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-4.pth.tar', 93.48491988689915)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-3.pth.tar', 89.24363807728558)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-2.pth.tar', 84.47219604147031)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-1.pth.tar', 83.01131008482564)\n",
            " ('drive/MyDrive/UNC/NCBG/output/20220904-184244-convnext_large_384_in22ft1k-600/checkpoint-0.pth.tar', 79.94816211121584)\n",
            "\n",
            "Train: 5 [   0/2122 (  0%)]  Loss: 0.3068 (0.307)  Time: 0.871s,    4.59/s  (0.871s,    4.59/s)  LR: 4.953e-05  Data: 0.464 (0.464)\n",
            "Train: 5 [  50/2122 (  2%)]  Loss: 0.3383 (0.487)  Time: 0.307s,   13.03/s  (0.370s,   10.82/s)  LR: 4.953e-05  Data: 0.009 (0.017)\n",
            "Train: 5 [ 100/2122 (  5%)]  Loss: 0.6267 (0.458)  Time: 0.286s,   13.98/s  (0.358s,   11.16/s)  LR: 4.953e-05  Data: 0.005 (0.012)\n",
            "Train: 5 [ 150/2122 (  7%)]  Loss: 0.4309 (0.448)  Time: 0.315s,   12.69/s  (0.340s,   11.77/s)  LR: 4.953e-05  Data: 0.007 (0.010)\n",
            "Train: 5 [ 200/2122 (  9%)]  Loss: 0.3664 (0.440)  Time: 0.329s,   12.18/s  (0.331s,   12.10/s)  LR: 4.953e-05  Data: 0.012 (0.009)\n",
            "Train: 5 [ 250/2122 ( 12%)]  Loss: 0.3377 (0.447)  Time: 0.297s,   13.46/s  (0.325s,   12.30/s)  LR: 4.953e-05  Data: 0.008 (0.009)\n",
            "Train: 5 [ 300/2122 ( 14%)]  Loss: 0.4284 (0.447)  Time: 0.302s,   13.24/s  (0.321s,   12.46/s)  LR: 4.953e-05  Data: 0.005 (0.008)\n",
            "Train: 5 [ 350/2122 ( 17%)]  Loss: 0.3357 (0.447)  Time: 0.292s,   13.72/s  (0.317s,   12.62/s)  LR: 4.953e-05  Data: 0.009 (0.008)\n",
            "Train: 5 [ 400/2122 ( 19%)]  Loss: 0.3009 (0.452)  Time: 0.289s,   13.85/s  (0.313s,   12.76/s)  LR: 4.953e-05  Data: 0.008 (0.008)\n",
            "Train: 5 [ 450/2122 ( 21%)]  Loss: 0.4945 (0.457)  Time: 0.287s,   13.96/s  (0.311s,   12.88/s)  LR: 4.953e-05  Data: 0.005 (0.007)\n",
            "Train: 5 [ 500/2122 ( 24%)]  Loss: 0.2977 (0.458)  Time: 0.288s,   13.88/s  (0.308s,   12.97/s)  LR: 4.953e-05  Data: 0.006 (0.007)\n",
            "Train: 5 [ 550/2122 ( 26%)]  Loss: 0.2954 (0.458)  Time: 0.286s,   14.01/s  (0.306s,   13.05/s)  LR: 4.953e-05  Data: 0.004 (0.007)\n",
            "Train: 5 [ 600/2122 ( 28%)]  Loss: 0.2981 (0.459)  Time: 0.288s,   13.87/s  (0.305s,   13.12/s)  LR: 4.953e-05  Data: 0.004 (0.007)\n",
            "Train: 5 [ 650/2122 ( 31%)]  Loss: 0.4448 (0.459)  Time: 0.287s,   13.95/s  (0.304s,   13.17/s)  LR: 4.953e-05  Data: 0.004 (0.007)\n",
            "Train: 5 [ 700/2122 ( 33%)]  Loss: 0.6178 (0.464)  Time: 0.287s,   13.96/s  (0.303s,   13.22/s)  LR: 4.953e-05  Data: 0.004 (0.007)\n",
            "Train: 5 [ 750/2122 ( 35%)]  Loss: 0.6386 (0.463)  Time: 0.289s,   13.83/s  (0.302s,   13.26/s)  LR: 4.953e-05  Data: 0.008 (0.007)\n",
            "Train: 5 [ 800/2122 ( 38%)]  Loss: 0.4985 (0.464)  Time: 0.287s,   13.95/s  (0.301s,   13.30/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [ 850/2122 ( 40%)]  Loss: 0.2986 (0.463)  Time: 0.290s,   13.78/s  (0.300s,   13.33/s)  LR: 4.953e-05  Data: 0.006 (0.006)\n",
            "Train: 5 [ 900/2122 ( 42%)]  Loss: 0.4511 (0.465)  Time: 0.289s,   13.82/s  (0.299s,   13.36/s)  LR: 4.953e-05  Data: 0.007 (0.006)\n",
            "Train: 5 [ 950/2122 ( 45%)]  Loss: 0.7878 (0.464)  Time: 0.286s,   14.00/s  (0.299s,   13.39/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1000/2122 ( 47%)]  Loss: 0.3897 (0.463)  Time: 0.291s,   13.75/s  (0.298s,   13.41/s)  LR: 4.953e-05  Data: 0.008 (0.006)\n",
            "Train: 5 [1050/2122 ( 50%)]  Loss: 0.3055 (0.461)  Time: 0.288s,   13.88/s  (0.298s,   13.43/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1100/2122 ( 52%)]  Loss: 0.3669 (0.461)  Time: 0.287s,   13.96/s  (0.297s,   13.45/s)  LR: 4.953e-05  Data: 0.004 (0.006)\n",
            "Train: 5 [1150/2122 ( 54%)]  Loss: 0.4410 (0.460)  Time: 0.289s,   13.86/s  (0.297s,   13.47/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1200/2122 ( 57%)]  Loss: 0.3897 (0.461)  Time: 0.290s,   13.80/s  (0.297s,   13.48/s)  LR: 4.953e-05  Data: 0.007 (0.006)\n",
            "Train: 5 [1250/2122 ( 59%)]  Loss: 0.3314 (0.462)  Time: 0.287s,   13.95/s  (0.296s,   13.49/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1300/2122 ( 61%)]  Loss: 0.6398 (0.461)  Time: 0.287s,   13.91/s  (0.296s,   13.51/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1350/2122 ( 64%)]  Loss: 0.6969 (0.462)  Time: 0.292s,   13.70/s  (0.296s,   13.52/s)  LR: 4.953e-05  Data: 0.008 (0.006)\n",
            "Train: 5 [1400/2122 ( 66%)]  Loss: 0.2952 (0.461)  Time: 0.288s,   13.88/s  (0.296s,   13.53/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1450/2122 ( 68%)]  Loss: 0.3303 (0.461)  Time: 0.286s,   14.01/s  (0.295s,   13.55/s)  LR: 4.953e-05  Data: 0.004 (0.006)\n",
            "Train: 5 [1500/2122 ( 71%)]  Loss: 0.8431 (0.463)  Time: 0.291s,   13.73/s  (0.295s,   13.55/s)  LR: 4.953e-05  Data: 0.007 (0.006)\n",
            "Train: 5 [1550/2122 ( 73%)]  Loss: 0.2958 (0.463)  Time: 0.287s,   13.95/s  (0.295s,   13.56/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1600/2122 ( 75%)]  Loss: 0.4699 (0.463)  Time: 0.295s,   13.56/s  (0.295s,   13.57/s)  LR: 4.953e-05  Data: 0.007 (0.006)\n",
            "Train: 5 [1650/2122 ( 78%)]  Loss: 0.6610 (0.462)  Time: 0.288s,   13.90/s  (0.294s,   13.58/s)  LR: 4.953e-05  Data: 0.004 (0.006)\n",
            "Train: 5 [1700/2122 ( 80%)]  Loss: 0.2930 (0.462)  Time: 0.288s,   13.89/s  (0.294s,   13.59/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n",
            "Train: 5 [1750/2122 ( 83%)]  Loss: 0.3453 (0.463)  Time: 0.287s,   13.94/s  (0.294s,   13.60/s)  LR: 4.953e-05  Data: 0.005 (0.006)\n"
          ]
        }
      ],
      "source": [
        "! python -u -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 ./pytorch-image-models/train.py qual --model convnext_large_384_in22ft1k --opt adabelief --lr 0.00005 --epochs 80 --decay-epochs 3 --cooldown-epochs 0 --weight-decay 1e-4 --sched cosine -b 4 --input-size 3 600 600 --num-classes=3 --vflip 0.5 --hflip 0.5 --amp --pretrained --output drive/MyDrive/UNC/NCBG/output/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMHrwfTNvpQI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1tIP185e2x7fME23ZYtWrBHUXkxpW0FpU",
      "authorship_tag": "ABX9TyMNIQoMNK2nvKeLa8UAmyS7",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}